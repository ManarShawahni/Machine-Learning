# ğŸ“Œ Regression Analysis and Model Selection

### *Machine Learning & Data Science - ENCS5341*

ğŸš— **Predicting Car Prices Using Regression Models**

---

## ğŸ“š **Project Overview**
This project explores **regression modeling techniques** to predict **car prices** using a dataset from [YallaMotors](https://www.kaggle.com/datasets/ahmedwaelnasef/cars-dataset/data). The dataset consists of **6,310 car listings** with various attributes such as **engine capacity, cylinders, horsepower, top speed, and seats**.

We implemented **linear and nonlinear regression models**, evaluated their performance, and optimized them using **feature selection, regularization, and hyperparameter tuning**.

---

## ğŸ“‚ **Repository Structure**
```
ğŸ“‚ Assignment_2_Regression_Models
â”‚â”€â”€ ğŸ“ README.md (This file)
â”‚â”€â”€ ğŸ“ Report.pdf (Detailed analysis and discussion)
â”‚â”€â”€ ğŸ“ ML_assignment_2_description.pdf (Assignment instructions and requirements)
â”‚â”€â”€ ğŸ“ Codes and datasets (Contains the code and dataset)
â”‚    â”‚â”€â”€ ğŸ“Š ML_assignment2.ipynb (Jupyter Notebook with code and analysis)
â”‚    â”‚â”€â”€ ğŸ“ dataset.csv (Preprocessed dataset)
```

---

## ğŸ” **Dataset Overview**
The dataset includes **9 features** extracted from YallaMotors, with key attributes such as:

| Feature Name         | Description |
|---------------------|-------------|
| `Car Name`         | Name and brand of the car |
| `Engine Capacity`  | Engine size in liters |
| `Cylinders`        | Number of cylinders |
| `Horsepower`       | Engine power output |
| `Top Speed`        | Maximum speed of the car |
| `Seats`            | Number of seats available |
| `Country`         | Country of sale |
| `Price`           | Car price in various currencies |

ğŸ“Œ **Note:** Prices were converted to **USD** for consistency.

---

## âš™ï¸ **Data Preprocessing & Feature Engineering**
### **ğŸ”¹ Data Cleaning & Handling Missing Values**
- **Converted prices to USD** using exchange rates.
- **Replaced outliers and non-numeric values** with NaN.
- **Filled missing values** using the median.
- **Encoded categorical features** (Brand, Country) using **Label Encoding**.
- **Standardized numerical features** using **StandardScaler**.

ğŸ“Œ The dataset was split into:
âœ… **60% Training**  
âœ… **20% Validation**  
âœ… **20% Test**

---

## ğŸ“Š **Exploratory Data Analysis (EDA)**
âœ”ï¸ **Histograms, Boxplots, and Correlation Matrix** were used to analyze data distribution and detect outliers.  
âœ”ï¸ **Feature importance analysis** was performed to identify the most relevant attributes for price prediction.

---

## ğŸ“ˆ **Regression Models Implemented**
### **ğŸ”¹ Linear Models**
1ï¸âƒ£ **Standard Linear Regression**  
2ï¸âƒ£ **LASSO (L1 Regularization)**  
3ï¸âƒ£ **Ridge Regression (L2 Regularization)**  

ğŸ“Œ **Key Findings:**  
- Ridge performed **better than LASSO**, with a lower MSE.
- **LASSO helped with feature selection** by setting some coefficients to zero.

---

### **ğŸ”¹ Advanced Regression Models**
4ï¸âƒ£ **Closed-Form Solution for Linear Regression**  
5ï¸âƒ£ **Gradient Descent for Linear Regression**  

ğŸ“Œ **Comparison:**  
- The **Closed-form solution** performed slightly better.
- **Gradient Descent** converged well but required tuning the learning rate.

---

### **ğŸ”¹ Nonlinear Models**
6ï¸âƒ£ **Radial Basis Function (RBF) Kernel Regression (SVR)**  
7ï¸âƒ£ **Polynomial Regression (Degree 2-10)**  

ğŸ“Œ **Key Findings:**  
- **SVR (RBF Kernel) outperformed all models**, achieving the lowest **MSE of 0.155**.  
- **Polynomial Regression (Degree 3)** also performed well, but higher degrees **(5-10)** led to overfitting.

---

## ğŸ¯ **Model Selection & Optimization**
### **ğŸ”¹ Feature Selection with Forward Selection**
- **Selected Features:** `Horsepower, Seats, Cylinders, Top Speed, Brand`
- **Reduced MSE to 0.2395**, confirming the importance of selected features.

### **ğŸ”¹ Regularization Techniques**
- **Ridge Regression (L2) performed better than LASSO**.
- **Best alpha values** found via **Grid Search**:
  - âœ… **Ridge Î± = 890.215**
  - âœ… **LASSO Î± = 0.0001**

### **ğŸ”¹ Hyperparameter Tuning (Grid Search)**
- **Optimized Polynomial Regression Degree = 5** (MSE **0.104**)
- **Best SVR parameters: C = 10, epsilon = 0.1** (MSE **0.137**)

ğŸ“Œ **Final Best Model:** **Polynomial Regression (Degree 5)**

---

## ğŸ› ï¸ **Technologies Used**
âœ… **Python**  
âœ… **Pandas, NumPy** (Data Processing)  
âœ… **Matplotlib, Seaborn** (Visualization)  
âœ… **Scikit-learn** (Machine Learning)  

---

## ğŸ“ **How to Run the Project**
1ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/ManarShawahni/Machine-Learning.git
```

2ï¸âƒ£ Navigate into the project directory:
```bash
cd Machine-Learning/Assignment_2_Regression_Models/Codes and datasets
```

3ï¸âƒ£ Open Jupyter Notebook:
```bash
jupyter notebook
```
- Open `ML_assignment2.ipynb`
- Run each cell sequentially to execute the analysis.

4ï¸âƒ£ **Alternatively, run Jupyter Notebook in VS Code:**
```bash
code .
```
- Open `ML_assignment2.ipynb` in **VS Code**.
- Ensure the **Jupyter extension** is installed.
- Install Jupyter Extension in VS Code:
  - Open **VS Code**.
  - Press **Ctrl + Shift + P**, search for **Extensions: Install Extensions**.
  - Search for **Jupyter** and install it.
- Click on **Run All** to execute the notebook.

5ï¸âƒ£ **Explore the analysis and modify as needed.**
